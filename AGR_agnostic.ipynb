{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AGR-agnostic.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPQSZyDFNfaL"
      },
      "source": [
        "# AGR-agnostic attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_O-70fSMe60"
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse, os, sys, csv, shutil, time, random, operator, pickle, ast, math, errno\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.optim import Optimizer\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torch.multiprocessing as mp\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnWFADZiP92H"
      },
      "source": [
        "#MIN-MAX attack\n",
        "def our_attack_dist(all_updates, model_re, n_attackers, dev_type='unit_vec', threshold=30):\n",
        "    \"\"\"\n",
        "    This attack is aim to generate the malicios updates such that its maximum \n",
        "    distance is capped at the maximum distance between any two benign gradients.\n",
        "    Input: \n",
        "    attack_tp: the type of attacks (min-sum,min-max)\n",
        "    all_updates: grediants of users\n",
        "    model_re: the global updates after aggregation\n",
        "    n_attackers: the number of attackers\n",
        "    dev_type: the type of disturbance variable\n",
        "\n",
        "    Output: \n",
        "    mal_update: malicious updates\n",
        "    \"\"\"\n",
        "\n",
        "    # Type of disturbance variable\n",
        "    if dev_type == 'unit_vec': # Inverse unit vector\n",
        "        # unit vector, direction opposite to good dir\n",
        "        deviation = model_re / torch.norm(model_re)\n",
        "\n",
        "    elif dev_type == 'sign': # Inverse sign\n",
        "        deviation = torch.sign(model_re) # output a tensor after sign function\n",
        "        # effective for classification task \n",
        "\n",
        "    elif dev_type == 'std': # Inverse standard deviation\n",
        "        deviation = torch.std(all_updates, 0) \n",
        "\n",
        "    lamda = torch.Tensor([10.0]).float().cuda()#initialize lamda \n",
        "    threshold_diff = 1e-5           #the threshold of lamda\n",
        "    lamda_fail = lamda\n",
        "    lamda_succ = 0\n",
        "    \n",
        "    #computer the distance of all benign uqdates\n",
        "    distances = []\n",
        "    for update in all_updates:\n",
        "        distance = torch.norm((all_updates - update), dim=1) ** 2\n",
        "        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
        "    \n",
        "    max_distance = torch.max(distances)# the maximum distance from all updates\n",
        "    del distances\n",
        "\n",
        "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
        "        mal_update = (model_re - lamda * deviation)\n",
        "        distance = torch.norm((all_updates - mal_update), dim=1) ** 2\n",
        "        max_d = torch.max(distance)\n",
        "        \n",
        "        if max_d <= max_distance:\n",
        "            # print('successful lamda is ', lamda)\n",
        "            lamda_succ = lamda\n",
        "            lamda = lamda + lamda_fail / 2\n",
        "        else:\n",
        "            lamda = lamda - lamda_fail / 2\n",
        "\n",
        "        lamda_fail = lamda_fail / 2\n",
        "\n",
        "    mal_update = (model_re - lamda_succ * deviation)\n",
        "    \n",
        "    return mal_update\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p0KS899KRdp"
      },
      "source": [
        "#MIN-SUM attack\n",
        "def our_attack_score(all_updates, model_re, n_attackers, dev_type='unit_vec', threshold=30):\n",
        "    \"\"\"\n",
        "    This attack is aim to generate the malicios updates such that its maximum \n",
        "    distance is capped at the maximum distance between any two benign gradients.\n",
        "    Input: \n",
        "    attack_tp: the type of attacks (min-sum,min-max)\n",
        "    all_updates: grediants of users\n",
        "    model_re: the global updates after aggregation\n",
        "    n_attackers: the number of attackers\n",
        "    dev_type: the type of disturbance variable\n",
        "\n",
        "    Output: \n",
        "    mal_update: malicious updates\n",
        "    \"\"\"\n",
        "\n",
        "    # Type of disturbance variable\n",
        "    if dev_type == 'unit_vec': # Inverse unit vector\n",
        "        # unit vector, direction opposite to good dir\n",
        "        deviation = model_re / torch.norm(model_re)\n",
        "\n",
        "    elif dev_type == 'sign': # Inverse sign\n",
        "        deviation = torch.sign(model_re) # output a tensor after sign function\n",
        "        # effective for classification task \n",
        "\n",
        "    elif dev_type == 'std': # Inverse standard deviation\n",
        "        deviation = torch.std(all_updates, 0) \n",
        "\n",
        "    lamda = torch.Tensor([10.0]).float().cuda()#initialize lamda \n",
        "    threshold_diff = 1e-5           #the threshold of lamda\n",
        "    lamda_fail = lamda\n",
        "    lamda_succ = 0\n",
        "    \n",
        "    #computer the distance of all benign uqdates\n",
        "    distances = []\n",
        "    for update in all_updates:\n",
        "        distance = torch.norm((all_updates - update), dim=1) ** 2\n",
        "        distances = distance[None, :] if not len(distances) else torch.cat((distances, distance[None, :]), 0)\n",
        "    \n",
        "    scores = torch.sum(distances, dim=1)\n",
        "    min_score = torch.min(scores)\n",
        "    del distances\n",
        "\n",
        "    while torch.abs(lamda_succ - lamda) > threshold_diff:\n",
        "        mal_update = (model_re - lamda * deviation)\n",
        "        distance = torch.norm((all_updates - mal_update), dim=1) ** 2\n",
        "        score = torch.sum(distance)\n",
        "        \n",
        "        if score <= min_score:\n",
        "            # print('successful lamda is ', lamda)\n",
        "            lamda_succ = lamda\n",
        "            lamda = lamda + lamda_fail / 2\n",
        "        else:\n",
        "            lamda = lamda - lamda_fail / 2\n",
        "\n",
        "        lamda_fail = lamda_fail / 2\n",
        "\n",
        "    # print(lamda_succ)\n",
        "    mal_update = (model_re - lamda_succ * deviation)\n",
        "    \n",
        "    return mal_update\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIBea4o-MlXE"
      },
      "source": [
        "# utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWdouXaQKh7K"
      },
      "source": [
        "#misc.py\n",
        "'''Some helper functions for PyTorch, including:\n",
        "    - get_mean_and_std: calculate the mean and std value of dataset.\n",
        "    - msr_init: net parameter initialization.\n",
        "    - progress_bar: progress bar mimic xlua.progress.\n",
        "'''\n",
        "import errno\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "from torch.autograd import Variable\n",
        "\n",
        "__all__ = ['get_mean_and_std', 'init_params', 'mkdir_p', 'AverageMeter']\n",
        "\n",
        "\n",
        "def get_mean_and_std(dataset):\n",
        "    '''Compute the mean and std value of dataset.'''\n",
        "    dataloader = trainloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "    print(dataloader)\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "\n",
        "    print('==> Computing mean and std..')\n",
        "    for inputs, targets in dataloader:\n",
        "        for i in range(3):\n",
        "            mean[i] += inputs[:,i,:,:].mean()\n",
        "            print(mean)\n",
        "            std[i] += inputs[:,i,:,:].std()\n",
        "            print(std)\n",
        "    mean.div_(len(dataset))\n",
        "    print(mean.div_(len(dataset)))\n",
        "    std.div_(len(dataset))\n",
        "    return mean, std\n",
        "\n",
        "def init_params(net):\n",
        "    '''Init layer parameters.'''\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            init.kaiming_normal(m.weight, mode='fan_out')\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            init.constant(m.weight, 1)\n",
        "            init.constant(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            init.normal(m.weight, std=1e-3)\n",
        "            if m.bias:\n",
        "                init.constant(m.bias, 0)\n",
        "\n",
        "def mkdir_p(path):\n",
        "    '''make dir if not exist'''\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError as exc:  # Python >2.5\n",
        "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
        "            pass\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\n",
        "       Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjq6-84sbQ_H"
      },
      "source": [
        "class SGD(Optimizer):\n",
        "    r\"\"\"Implements stochastic gradient descent (optionally with momentum).\n",
        "\n",
        "    Nesterov momentum is based on the formula from\n",
        "    `On the importance of initialization and momentum in deep learning`__.\n",
        "\n",
        "    Args:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float): learning rate\n",
        "        momentum (float, optional): momentum factor (default: 0)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "        dampening (float, optional): dampening for momentum (default: 0)\n",
        "        nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
        "\n",
        "    Example:\n",
        "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "        >>> optimizer.zero_grad()\n",
        "        >>> loss_fn(model(input), target).backward()\n",
        "        >>> optimizer.step()\n",
        "\n",
        "    __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n",
        "\n",
        "    .. note::\n",
        "        The implementation of SGD with Momentum/Nesterov subtly differs from\n",
        "        Sutskever et. al. and implementations in some other frameworks.\n",
        "\n",
        "        Considering the specific case of Momentum, the update can be written as\n",
        "\n",
        "        .. math::\n",
        "                  v = \\rho * v + g \\\\\n",
        "                  p = p - lr * v\n",
        "\n",
        "        where p, g, v and :math:`\\rho` denote the parameters, gradient,\n",
        "        velocity, and momentum respectively.\n",
        "\n",
        "        This is in contrast to Sutskever et. al. and\n",
        "        other frameworks which employ an update of the form\n",
        "\n",
        "        .. math::\n",
        "             v = \\rho * v + lr * g \\\\\n",
        "             p = p - v\n",
        "\n",
        "        The Nesterov version is analogously modified.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr, momentum=0, dampening=0,\n",
        "                 weight_decay=0, nesterov=False):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
        "                        weight_decay=weight_decay, nesterov=nesterov)\n",
        "        if nesterov and (momentum <= 0 or dampening != 0):\n",
        "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
        "        super(SGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SGD, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    def step(self, grads, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            dampening = group['dampening']\n",
        "            nesterov = group['nesterov']\n",
        "\n",
        "            for i,p in enumerate(group['params']):\n",
        "#                 if p.grad is None:\n",
        "#                     continue\n",
        "                \n",
        "                d_p = grads[i]\n",
        "                \n",
        "                if weight_decay != 0:\n",
        "                    d_p.add_(weight_decay, p.data)\n",
        "                if momentum != 0:\n",
        "                    param_state = self.state[p]\n",
        "                    if 'momentum_buffer' not in param_state:\n",
        "                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
        "                    else:\n",
        "                        buf = param_state['momentum_buffer']\n",
        "                        buf.mul_(momentum).add_(1 - dampening, d_p)\n",
        "                    if nesterov:\n",
        "                        d_p = d_p.add(momentum, buf)\n",
        "                    else:\n",
        "                        d_p = buf\n",
        "\n",
        "                p.data.add_(-group['lr'], d_p)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y68PAgbodorT"
      },
      "source": [
        "#eval.py\n",
        "from __future__ import print_function, absolute_import\n",
        "\n",
        "__all__ = ['accuracy']\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBlQsxUdNBD9"
      },
      "source": [
        "#models and train test function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRgonxbcNZkl"
      },
      "source": [
        "# models\n",
        "from __future__ import print_function\n",
        "import argparse, os, sys, csv, shutil, time, random, operator, pickle, ast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import cifar as models\n",
        "\n",
        "class cifar_mlp(nn.Module):\n",
        "    def __init__(self, ninputs=3 * 32 * 32, num_classes=10):\n",
        "        self.ninputs = ninputs\n",
        "        self.num_classes = num_classes\n",
        "        super(cifar_mlp, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Linear(self.ninputs, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 64),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.classifier = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.ninputs)\n",
        "        hidden_out = self.features(x)\n",
        "        return self.classifier(hidden_out)\n",
        "\n",
        "\n",
        "def get_model(config, parallel=False, cuda=True, device=0):\n",
        "    # print(\"==> creating model '{}'\".format(config['arch']))\n",
        "    if config['arch'].startswith('resnext'):\n",
        "        model = models.__dict__[config['arch']](\n",
        "            cardinality=config['cardinality'],\n",
        "            num_classes=config['num_classes'],\n",
        "            depth=config['depth'],\n",
        "            widen_factor=config['widen-factor'],\n",
        "            dropRate=config['drop'],\n",
        "        )\n",
        "    elif config['arch'].startswith('densenet'):\n",
        "        model = models.__dict__[config['arch']](\n",
        "            num_classes=config['num_classes'],\n",
        "            depth=config['depth'],\n",
        "            growthRate=config['growthRate'],\n",
        "            compressionRate=config['compressionRate'],\n",
        "            dropRate=config['drop'],\n",
        "        )\n",
        "    elif config['arch'].startswith('wrn'):\n",
        "        model = models.__dict__[config['arch']](\n",
        "            num_classes=config['num_classes'],\n",
        "            depth=config['depth'],\n",
        "            widen_factor=config['widen-factor'],\n",
        "            dropRate=config['drop'],\n",
        "        )\n",
        "    elif config['arch'].endswith('resnet'):\n",
        "        model = models.__dict__[config['arch']](\n",
        "            num_classes=config['num_classes'],\n",
        "            depth=config['depth'],\n",
        "        )\n",
        "    elif config['arch'].endswith('convnet'):\n",
        "        model = models.__dict__[config['arch']](\n",
        "            num_classes=config['num_classes']\n",
        "        )\n",
        "    else:\n",
        "        model = models.__dict__[config['arch']](num_classes=config['num_classes'], )\n",
        "\n",
        "    if parallel:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    if cuda:\n",
        "        model.cuda()\n",
        "\n",
        "    return model\n",
        "\n",
        "def return_model(model_name, lr, momentum, parallel=False, cuda=True, device=0):\n",
        "    if model_name == 'dc':\n",
        "        arch_config = {\n",
        "            'arch': 'Dc',\n",
        "            'num_classes': 10,\n",
        "        }\n",
        "        model = get_model(arch_config, parallel=parallel, cuda=cuda, device=device)\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=momentum)\n",
        "\n",
        "    elif model_name == 'alexnet':\n",
        "        arch_config = {\n",
        "            'arch': 'alexnet',\n",
        "            'num_classes': 10,\n",
        "        }\n",
        "        model = get_model(arch_config, parallel=parallel, cuda=cuda, device=device)\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=momentum)\n",
        "    elif model_name == 'densenet-bc-100-12':\n",
        "        arch_config = {\n",
        "            'arch': 'densenet',\n",
        "            'depth': 100,\n",
        "            'growthRate': 12,\n",
        "            'compressionRate': 2,\n",
        "            'drop': 0,\n",
        "            'num_classes': 10,\n",
        "        }\n",
        "        model = get_model(arch_config, parallel=parallel, cuda=cuda, device=device)\n",
        "        # optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=momentum,weight_decay=1e-4)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    elif model_name == 'densenet-bc-L190-k40':\n",
        "        arch_config = {\n",
        "            'arch': 'densenet',\n",
        "            'depth': 190,\n",
        "            'growthRate': 40,\n",
        "            'compressionRate': 2,\n",
        "            'drop': 0,\n",
        "            'num_classes': 10,\n",
        "        }\n",
        "        model = get_model(arch_config, parallel=parallel, cuda=cuda, device=device)\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=1e-4)\n",
        "    elif model_name == 'preresnet-110':\n",
        "        arch_config = {\n",
        "            'arch': 'preresnet',\n",
        "            'depth': 110,\n",
        "            'num_classes': 10,\n",
        "        }\n",
        "        model = get_model(arch_config, parallel=parallel, cuda=cuda, device=device)\n",
        "        # optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=momentum, weight_decay=1e-4)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    elif model_name == 'resnet-110':\n",
        "        arch_config = {\n",
        "            'arch': 'resnet',\n",
        "            'depth': 110,\n",
        "            'num_classes': 10,\n",
        "        }\n",
        "        model = get_model(arch_config, parallel=parallel, cuda=cuda, device=device)\n",
        "        # optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=1e-4)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    elif model_name == 'resnext-16x64d':\n",
        "        arch_config = {\n",
        "            'arch': 'resnext',\n",
        "            'depth': 29,\n",
        "            'cardinality': 16,\n",
        "            'widen-factor': 4,\n",
        "            'drop': 0,\n",
        "            'num_classes': 10,\n",
        "        }\n",
        "        model = get_model(arch_config, parallel=parallel, cuda=cuda, device=device)\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=5e-4)\n",
        "    elif model_name == 'resnext-8x64d':\n",
        "        arch_config = {\n",
        "            'arch': 'resnext',\n",
        "            'depth': 29,\n",
        "            'cardinality': 8,\n",
        "            'widen-factor': 4,\n",
        "            'drop': 0,\n",
        "            'num_classes': 10,\n",
        "        }\n",
        "        model = get_model(arch_config, parallel=parallel, cuda=cuda, device=device)\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=5e-4)\n",
        "    elif model_name.startswith('vgg'):\n",
        "        arch_config = {\n",
        "            'arch': model_name,\n",
        "            'num_classes': 10,\n",
        "        }\n",
        "        model = get_model(arch_config, parallel=parallel, cuda=cuda, device=device)\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "    elif model_name == 'WRN-28-10-drop':\n",
        "        arch_config = {\n",
        "            'arch': 'wrn',\n",
        "            'depth': 28,\n",
        "            'widen-factor': 10,\n",
        "            'drop': 0.3,\n",
        "            'num_classes': 10,\n",
        "        }\n",
        "        model = get_model(arch_config, parallel=parallel, cuda=cuda, device=device)\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=5e-4)\n",
        "    else:\n",
        "        assert (False), 'Model not found!'\n",
        "\n",
        "    return model, optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAeUvAclNuh3"
      },
      "source": [
        "#Alexnet\n",
        "'''AlexNet for CIFAR10. FC layers are removed. Paddings are adjusted.\n",
        "Without BN, the start learning rate should be 0.01\n",
        "(c) YANG, Wei \n",
        "'''\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "__all__ = ['alexnet']\n",
        "\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=5),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def alexnet(**kwargs):\n",
        "    r\"\"\"AlexNet model architecture from the\n",
        "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
        "    \"\"\"\n",
        "    model = AlexNet(**kwargs)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOZ9n8uNHwzJ"
      },
      "source": [
        "def train(train_data, labels, model, criterion, optimizer, use_cuda, num_batchs=999999, debug_='MEDIUM', batch_size=16):\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    end = time.time()\n",
        "    len_t = (len(train_data) // batch_size) - 1\n",
        "\n",
        "    for ind in range(len_t):\n",
        "        if ind > num_batchs:\n",
        "            break\n",
        "        # measure data loading time\n",
        "        inputs = train_data[ind * batch_size:(ind + 1) * batch_size]\n",
        "        targets = labels[ind * batch_size:(ind + 1) * batch_size]\n",
        "\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        if use_cuda:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
        "\n",
        "        # compute output\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
        "        losses.update(loss.item(), inputs.size(0))\n",
        "        top1.update(prec1.item(), inputs.size(0))\n",
        "        top5.update(prec5.item(), inputs.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # plot progress\n",
        "        if debug_ == 'HIGH' and ind % 100 == 0:\n",
        "            print('Classifier: ({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
        "                batch=ind + 1,\n",
        "                size=len_t,\n",
        "                data=data_time.avg,\n",
        "                bt=batch_time.avg,\n",
        "                loss=losses.avg,\n",
        "                top1=top1.avg,\n",
        "                top5=top5.avg,\n",
        "            ))\n",
        "\n",
        "    return (losses.avg, top1.avg)\n",
        "\n",
        "\n",
        "def test(test_data, labels, model, criterion, use_cuda, debug_='MEDIUM', batch_size=64):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    top5 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    len_t = (len(test_data) // batch_size) - 1\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for ind in range(len_t):\n",
        "            # measure data loading time\n",
        "            inputs = test_data[ind * batch_size:(ind + 1) * batch_size]\n",
        "            targets = labels[ind * batch_size:(ind + 1) * batch_size]\n",
        "\n",
        "            data_time.update(time.time() - end)\n",
        "\n",
        "            if use_cuda:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
        "\n",
        "            # compute output\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
        "            losses.update(loss.item(), inputs.size(0))\n",
        "            top1.update(prec1.item(), inputs.size(0))\n",
        "            top5.update(prec5.item(), inputs.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            # plot progress\n",
        "            if debug_ == 'HIGH' and ind % 100 == 0:\n",
        "                print('Test classifier: ({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
        "                    batch=ind + 1,\n",
        "                    size=len(test_data),\n",
        "                    data=data_time.avg,\n",
        "                    bt=batch_time.avg,\n",
        "                    loss=losses.avg,\n",
        "                    top1=top1.avg,\n",
        "                    top5=top5.avg,\n",
        "                ))\n",
        "\n",
        "    return (losses.avg, top1.avg)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRwyocsTMyAx"
      },
      "source": [
        "#download cifar10 train and test dataset \n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "data_loc='/mnt/nfs/work1/amir/vshejwalkar/cifar10_data/'\n",
        "# load the train dataset\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "cifar10_train = datasets.CIFAR10(root=data_loc, train=True, download=True, transform=train_transform)\n",
        "\n",
        "cifar10_test = datasets.CIFAR10(root=data_loc, train=False, download=True, transform=train_transform)\n",
        "\n",
        "X=[]\n",
        "Y=[]\n",
        "for i in range(len(cifar10_train)):\n",
        "    X.append(cifar10_train[i][0].numpy())\n",
        "    Y.append(cifar10_train[i][1])\n",
        "\n",
        "for i in range(len(cifar10_test)):\n",
        "    X.append(cifar10_test[i][0].numpy())\n",
        "    Y.append(cifar10_test[i][1])\n",
        "\n",
        "X=np.array(X)\n",
        "Y=np.array(Y)\n",
        "\n",
        "print('total data len: ',len(X))\n",
        "\n",
        "if not os.path.isfile('./cifar10_shuffle.pkl'):\n",
        "    all_indices = np.arange(len(X))\n",
        "    np.random.shuffle(all_indices)\n",
        "    pickle.dump(all_indices,open('./cifar10_shuffle.pkl','wb'))\n",
        "else:\n",
        "    all_indices=pickle.load(open('./cifar10_shuffle.pkl','rb'))\n",
        "\n",
        "X=X[all_indices]\n",
        "Y=Y[all_indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cLFOcoTH-Dg"
      },
      "source": [
        "# data loading\n",
        "\n",
        "nusers=50\n",
        "user_tr_len=1000\n",
        "\n",
        "total_tr_len=user_tr_len*nusers\n",
        "val_len=5000\n",
        "te_len=5000\n",
        "\n",
        "print('total data len: ',len(X))\n",
        "\n",
        "if not os.path.isfile('./cifar10_shuffle.pkl'):\n",
        "    all_indices = np.arange(len(X))\n",
        "    np.random.shuffle(all_indices)\n",
        "    pickle.dump(all_indices,open('./cifar10_shuffle.pkl','wb'))\n",
        "else:\n",
        "    all_indices=pickle.load(open('./cifar10_shuffle.pkl','rb'))\n",
        "\n",
        "total_tr_data=X[:total_tr_len]\n",
        "total_tr_label=Y[:total_tr_len]\n",
        "\n",
        "val_data=X[total_tr_len:(total_tr_len+val_len)]\n",
        "val_label=Y[total_tr_len:(total_tr_len+val_len)]\n",
        "\n",
        "te_data=X[(total_tr_len+val_len):(total_tr_len+val_len+te_len)]\n",
        "te_label=Y[(total_tr_len+val_len):(total_tr_len+val_len+te_len)]\n",
        "\n",
        "total_tr_data_tensor=torch.from_numpy(total_tr_data).type(torch.FloatTensor)\n",
        "total_tr_label_tensor=torch.from_numpy(total_tr_label).type(torch.LongTensor)\n",
        "\n",
        "val_data_tensor=torch.from_numpy(val_data).type(torch.FloatTensor)\n",
        "val_label_tensor=torch.from_numpy(val_label).type(torch.LongTensor)\n",
        "\n",
        "te_data_tensor=torch.from_numpy(te_data).type(torch.FloatTensor)\n",
        "te_label_tensor=torch.from_numpy(te_label).type(torch.LongTensor)\n",
        "\n",
        "print('total tr len %d | val len %d | test len %d'%(len(total_tr_data_tensor),len(val_data_tensor),len(te_data_tensor)))\n",
        "\n",
        "#==============================================================================================================\n",
        "\n",
        "user_tr_data_tensors=[]\n",
        "user_tr_label_tensors=[]\n",
        "\n",
        "for i in range(nusers):\n",
        "    \n",
        "    user_tr_data_tensor=torch.from_numpy(total_tr_data[user_tr_len*i:user_tr_len*(i+1)]).type(torch.FloatTensor)\n",
        "    user_tr_label_tensor=torch.from_numpy(total_tr_label[user_tr_len*i:user_tr_len*(i+1)]).type(torch.LongTensor)\n",
        "\n",
        "    user_tr_data_tensors.append(user_tr_data_tensor)\n",
        "    user_tr_label_tensors.append(user_tr_label_tensor)\n",
        "    print('user %d tr len %d'%(i,len(user_tr_data_tensor)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_NiKvxdDqXh"
      },
      "source": [
        "#Execute AGR-agnostic attack \n",
        "#Note: this code included two type attacks: min-max and min-sum\n",
        "\n",
        "batch_size=250\n",
        "resume=0\n",
        "nepochs=1200\n",
        "schedule=[1000]\n",
        "nbatches = user_tr_len//batch_size\n",
        "\n",
        "gamma=.5\n",
        "opt = 'sgd'\n",
        "fed_lr=0.5\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "aggregation='median'\n",
        "multi_k = False\n",
        "candidates = []\n",
        "\n",
        "at_type='min-max'# for AGR-agnostic, min-max has better impact\n",
        "dev_type ='std'\n",
        "threshold=10\n",
        "partial_attackers = {4:1, 5:1, 8:2, 10:3, 12:4}\n",
        "n_attackers=[10]\n",
        "\n",
        "arch='alexnet'\n",
        "chkpt='./'+aggregation\n",
        "\n",
        "for n_attacker in n_attackers:\n",
        "    candidates = []\n",
        "\n",
        "    epoch_num = 0\n",
        "    best_global_acc = 0\n",
        "    best_global_te_acc = 0\n",
        "\n",
        "    fed_model, _ = return_model(arch, 0.1, 0.9, parallel=False)\n",
        "    optimizer_fed = SGD(fed_model.parameters(), lr=fed_lr)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    r=np.arange(user_tr_len)\n",
        "\n",
        "    while epoch_num <= nepochs:\n",
        "        user_grads=[]\n",
        "        if not epoch_num and epoch_num%nbatches == 0:\n",
        "            np.random.shuffle(r)\n",
        "            for i in range(nusers):\n",
        "                user_tr_data_tensors[i]=user_tr_data_tensors[i][r]\n",
        "                user_tr_label_tensors[i]=user_tr_label_tensors[i][r]\n",
        "\n",
        "        for i in range(n_attacker, nusers):\n",
        "\n",
        "            inputs = user_tr_data_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
        "            targets = user_tr_label_tensors[i][(epoch_num%nbatches)*batch_size:((epoch_num%nbatches) + 1) * batch_size]\n",
        "\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
        "\n",
        "            outputs = fed_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            fed_model.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "            param_grad=[]\n",
        "            for param in fed_model.parameters():\n",
        "                param_grad=param.grad.data.view(-1) if not len(param_grad) else torch.cat((param_grad,param.grad.view(-1)))\n",
        "\n",
        "            user_grads=param_grad[None, :] if len(user_grads)==0 else torch.cat((user_grads,param_grad[None,:]), 0)\n",
        "\n",
        "        if epoch_num in schedule:\n",
        "            for param_group in optimizer_fed.param_groups:\n",
        "                param_group['lr'] *= gamma\n",
        "                print('New learnin rate ', param_group['lr'])\n",
        "\n",
        "        if n_attacker > 0:\n",
        "            if at_type == 'min-max':\n",
        "                n_attacker_ = partial_attackers[n_attacker]\n",
        "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
        "                mal_update = our_attack_dist(user_grads[:n_attacker], agg_grads, n_attacker_, threshold=threshold, dev_type=dev_type)\n",
        "            elif at_type == 'min-sum':\n",
        "                n_attacker_ = partial_attackers[n_attacker]\n",
        "                agg_grads = torch.mean(user_grads[:n_attacker], 0)\n",
        "                mal_update = our_attack_score(user_grads[:n_attacker], agg_grads, n_attacker_, threshold=threshold, dev_type=dev_type)\n",
        "\n",
        "            mal_updates = torch.stack([mal_update] * n_attacker)\n",
        "            malicious_grads = torch.cat((mal_updates, user_grads), 0)\n",
        "\n",
        "        if epoch_num==0: print('malicious_grads shape ', malicious_grads.shape)\n",
        "\n",
        "        # implement your aggregation here, median for example\n",
        "\n",
        "        agg_grads=torch.median(malicious_grads,dim=0)[0]\n",
        "\n",
        "        del user_grads\n",
        "\n",
        "        start_idx=0\n",
        "\n",
        "        optimizer_fed.zero_grad()\n",
        "\n",
        "        model_grads=[]\n",
        "\n",
        "        for i, param in enumerate(fed_model.parameters()):\n",
        "            param_=agg_grads[start_idx:start_idx+len(param.data.view(-1))].reshape(param.data.shape)\n",
        "            start_idx=start_idx+len(param.data.view(-1))\n",
        "            param_=param_.cuda()\n",
        "            model_grads.append(param_)\n",
        "\n",
        "        optimizer_fed.step(model_grads)\n",
        "\n",
        "        val_loss, val_acc = test(val_data_tensor,val_label_tensor,fed_model,criterion,use_cuda)\n",
        "        te_loss, te_acc = test(te_data_tensor,te_label_tensor, fed_model, criterion, use_cuda)\n",
        "\n",
        "        is_best = best_global_acc < val_acc\n",
        "\n",
        "        best_global_acc = max(best_global_acc, val_acc)\n",
        "\n",
        "        if is_best:\n",
        "            best_global_te_acc = te_acc\n",
        "\n",
        "        if epoch_num%25==0 or epoch_num==nepochs-1:\n",
        "            print('%s: at %s n_at %d | e %d fed_model val loss %.4f val acc %.4f best val_acc %f te_acc %f'%(aggregation, at_type, n_attacker, epoch_num, val_loss, val_acc, best_global_acc,best_global_te_acc))\n",
        "\n",
        "        if val_loss > 1000:\n",
        "            print('val loss %f too high'%val_loss)\n",
        "            break\n",
        "            \n",
        "        epoch_num+=1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}